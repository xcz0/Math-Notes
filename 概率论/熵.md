# 熵

熵 (entropy) 是表示随机变量不确定性的度量。

设 $X$ 是一个取有限个值的离散随机变量，其概率分布为
$$ P(X=x_i)=p_{i} $$
定义随机变量 $X$ 的**熵**为：
$$ H(X)=-\sum_{i=1}^{n}p_{i} \log p_{i} $$
通常，式中的对数以 2 为底或以 e 为底（自然对数），这时熵的单位分别称作比特(bit) 或纳特 (nat) 。

- [ ] 验证 $H(X)<\log n$

## 不确定性

若X服从0-1分布，则其熵为
$$ H(X)=-p \log p -(1-p) \log (1-p) $$

对p求导可得-\log p + \log(1-p)，即当p=0.5时，熵最大，此时随机变量不确定性也最大。而当p=0或p=1时，随机变量完全没有不确定性。

## 条件熵

条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。随机变量X 给定的条件下随机变量 Y 的**条件熵** (conditional entropy) H(YIX) ，定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望：
$$ H(Y|X)=E[H(Y|X)]=\sum_{i=1}^{n}H(Y|X=x_{i})p_{i}  $$

当墒和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的墒与条件熵分别称为**经验熵**(empirical entropy) 和**经验条件熵**(empirical conditional entropy) 。一般地，熵 H(Y) 与条件熵 H(Y IX) 之差称为**互信息** (mutual information) 。