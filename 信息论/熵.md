# 熵

熵 (entropy) 是表示随机变量不确定性的度量。

设 $X$ 是一个取有限个值的*离散*随机变量，其概率分布为
$$ P(X=x_i)=p_{i} $$
定义随机变量 $X$ 的**熵**为：
$$ H(X)=-\sum_{i=1}^{n}p_{i} \log p_{i} $$
通常，式中的对数以 2 为底或以 e 为底（自然对数），这时熵的单位分别称作**比特**(bit) 或**纳特** (nat) 。

- [ ] 验证 $H(X)<\log n$

## 不确定性

若X服从0-1分布，则其熵为
$$ H(X)=-p \log p -(1-p) \log (1-p) $$

对p求导可得 $-\log p + \log(1-p)$，即当 $p=0.5$ 时，熵最大，此时随机变量不确定性也最大。而当 $p=0$ 或 $p=1$ 时，随机变量完全没有不确定性。
