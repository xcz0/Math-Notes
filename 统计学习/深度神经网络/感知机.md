## 感知机模型

### 模型定义

> [!note] 感知机定义
> 设输入空间（特征空间）为$\mathcal{X} \subseteq R^n$，输出空间为$\mathcal{Y} =\{+1,-1\}$，$x \in \mathcal{X}$与$y \in \mathcal{Y}$分别表示输入输出的实例。并且输入空间到输出空间的映射为： 
> $$f(x)=\text{sign}(w \cdot x+b)$$
> 则称此为感知机模型。其中，$w$和$b$为感知机模型参数，$w \in \mathbb{R}^n$称为权值(weight)或权值向量(weight vector )，$b \in \mathbb{R}$称为偏置 ( bias )，$w\cdot x$ 表示 $w$ 和 $x$ 的内积。$\text{sign}()$ 是符号函数。

感知机是一种线性分类模型 ， 属于判别模型。感知机模型的假设空间是定义在
特征空间中的所有线性分类模型( linear classification model ) 或线性分类器( linear
classifier)， 即函数集合 $\{f|f(x) = w \cdot x + b\}$ 。

### 几何解释

线性方程$w \cdot x + b=0$对应特征空间$\mathbb{R}^n$中的一个超平面$S$其中 $w$ 是超平面的法向量 ，$b$ 是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类 。因此 ，称其为分离超平面 (separating hyperplane)。

![[Pasted image 20230216155111.png]]

## 感知机学习策略

>[!note] 线性可分定义
>给定一个数据集
> $$ T=\{ (x_{1},y_{1}),\dots,(x_{N},y_{N}) \} $$
> 其中，$x_{i} \in \mathcal{X}=\mathbb{R}^n,\ y_{i} \in \{ 1,-1 \}$。
> 如果存在某个超平面
> $$S:w \cdot x +b=0$$
> 能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即$y_{i}(w \cdot x_{i} + b)>0$，则称数据集 $T$ 为线性可分数据集 ( linearly separable data set ) ；否则，称数据集 $T$ 线性不可分。

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。损失函数的一个自然选择是误分类点的总数 。 但是，这样的损失函数不是参数 $w,b$ 的连续可导函数，不易优化。感知机所采用的损失函数是误分类点到超平面 S 的总距离。

输入空间$\mathbb{R}^n$中任一点$x_{0}$到超平面$S$ 的距离为：
$$ \frac{1}{\left \| w \right \|_{2}} |w \cdot x_{0} + b|$$
由于对于误分类点$(x_{i},y_{i})$而言，$-y_{i}(w \cdot x_{i} + b)>0$，故其到$S$的距离为
$$ -\frac{1}{\left \| w \right \|_{2}}y_{i}(w \cdot x_{i} + b)$$

为使记号简洁，令所有输入的第0个特征为1，即$\hat{x_{i}}=(1,x_{i})$，并且将权值与偏重合并为$\hat{w}=(w,b)$，则$(w \cdot x_{i} + b)=\hat{w} \cdot \hat{x_{i}}$。在不引起歧义的时候，直接将$\hat{x_{i}},\hat{w}$简记为$x_{i},w$，则模型改写为
$$f(x)=\text{sign}(w \cdot x)$$

## 感知机学习算法

### 原始形式

采用在线随机梯度下降法 (stochastic gradient descent) ，一次随机选取一个误分类点使其梯度下降。

> [!tip] 感知机学习算法
> 输入：训练集$T=\{ (x_{1},y_{1}),\dots,(x_{N},y_{N}) \},\ x_{i} \in \mathcal{X}=\mathbb{R}^n,\ y_{i} \in \{ 1,-1 \}$；学习率$0<\eta \leq 1$
> 输出：模型参数$w$
> 1. 选取初值$w_0$
> 2. 训练集中随机选取误分类数据$(x_i,y_i),y_i(w_{t}\cdot x_i)\leq 0$
> 3. 更新参数$$w_{t+1} \leftarrow w_{t} + \eta y_i x_i$$
> 4. 转至第2步，直至训练集中没有误分类点

由于
$$ y_{i} w_{t+1} x_i = y_{i} w_{t+1} x_i + \eta y_i^2 x_i\cdot x_i  \geq y_{i} w_{t} x_i $$
即更新后对于此点的损失函数更小。直观上有如下解释 ： 当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整 w 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

### 算法的收敛性

>[!note] Novikoff定理
>设训练数据集$T$线性可分，则：
>1. 存在$|w_{opt}|=1$的超平面$S_{opt}:w_{opt} \cdot x=0$将训练数据集完全正确分开；且存在$\gamma>0$, 对所有的数据有
> $$ y_i(w_{opt} \cdot x_i) \geq \gamma $$
>2. 令$R = max|x_{i}|$，则感知机算法在训练数据集上的误分类次数 $k$不超过定值
> $$ k \leq \left( \frac{R}{\gamma} \right)^2 $$

由于$T$线性可分，故存在超平面$S:w \cdot x=0$将训练数据集完全正确分开。只要将其参数进行归一化处理$w_{opt}=\frac{w}{|w|}$即可，并且$S_{opt}=S$。同时，由于$S_{opt}$将训练数据集完全正确分开，即$y_{i}w_{opt}x_{i}>0$。令$\gamma=\min_{t}\{ y_{i}w_{opt}x_{i} \}>0$，则有
$$ y_i <w_{opt} ,x_i> \geq \gamma $$

因为
$$ <w_{k}, w_{opt}> = <w_{k-1}+\eta y_i x_i, w_{opt}> = <w_{k-1}, w_{opt}> +\eta y_i <x_i,w_{opt}> \geq <w_{k-1}, w_{opt}>+\eta \gamma $$
递推可得：
$$ <w_{k}, w_{opt}> \geq k\eta \gamma  $$
又
$$ |w_{k}|^2=|w_{k-1}|^2+2 \eta y_{i} w_{k-1} x_{i} + \eta^2 |x_{i}|^2 \leq |w_{k-1}|^2 + \eta^2 |x_{i}|^2 \leq |w_{k-1}|^2 + \eta^2 R^2 $$
递推可得：
$$ |w_{k}|^2 \geq k \eta^2 R^2 $$
综上
$$k\eta \gamma \leq <w_{k}, w_{opt}> \leq |w_{k}||w_{opt}| \leq \sqrt{k} \eta R   $$
于是
$$ k \leq \left( \frac{R}{\gamma} \right)^2 $$

### 对偶形式

注意到，若令$w_{0}=0$，则参数恒为实例的线性组合
$$ w=\eta\sum_{i=1}^{N}n_{i} y_i x_{i} $$
其中$n_{i}$代表第 i 个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响最大。

> [!tip] 感知机学习算法
> 输入：训练集$T=\{ (x_{1},y_{1}),\dots,(x_{N},y_{N}) \},\ x_{i} \in \mathcal{X}=\mathbb{R}^n,\ y_{i} \in \{ 1,-1 \}$；学习率$0<\eta \leq 1$
> 输出：参数$(n_{1},\cdots ,n_{N})$
> 1. $(n_{1},\cdots ,n_{N}) \leftarrow (0,\cdots,0)​$
> 2. 训练集中随机选取误分类数据$(x_i,y_i),\ y_i(\sum_{j=1}^{N}n_{j} y_j x_{i}\cdot x_i)\leq 0$
> 3. 更新参数$$n_{i} \leftarrow n_{i} + 1$$
> 4. 转至第2步，直至训练集中没有误分类点

在算法中，训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的 Gram 矩阵 (Gram matrix)
$$ \mathbf{G} = [x_{i} \cdot x_{j}]_{N\times N} $$

## 线性可分

### 反例：异或

对于异或模型，输入与输出的映射关系为：

| $x_{1}$ | $x_{2}$ |  $y$  |    $y(wx+b)$     |
| :-----: | :-----: | :---: | :--------------: |
|    0    |    0    |  -1   |       $-b$       |
|    0    |    1    |   1   |    $w_{2}+b$     |
|    1    |    0    |   1   |    $w_{1}+b$     |
|    1    |    1    |  -1   | $-w_{1}-w_{2}-b$ |

若其线性可分，则要求
$$ 
\begin{cases}
-b > 0 \\ 
w_{2}+b > 0\\ 
w_{1}+b > 0 \\
-w_{1}-w_{2}-b > 0
\end{cases} 
$$
从第一个等式可推得$b<0$，而将后三个等式相加可得$b>0$，故此方程组无解，模型线性不可分。

### 线性可分的充分必要条件

若$S$为$\mathbb{R}^n$中$k$个点组成的点集，即$S=\{ x_{1},x_{2},\cdots,x_{k} \},\ x_{i} \in \mathbb{R}^n$，则其凸壳$\text{conv} (S)$为
$$ \text{conv} (S)=\left\{  x=\sum_{i=1}^{k} \lambda_{i} x_{i} \vert \sum_{i=1}^{k} \lambda_{i}=1 ,\ \lambda_{i}\geq 0  \right\} $$


>[!note] 线性可分的充分必要条件
>样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交。

**必要性**证明：线性可分$\Rightarrow$凸壳不相交 

设数据集$T$中的正例点集为$S_+$，凸壳为$\text{conv}(S_+)$，负实例点集为$S_-$，凸壳为$\text{conv}(S_-)$。假设样本集线性可分，则存在一个超平面$S:w \cdot x + b = 0$，使得：
$$
\begin{gather*}
   w \cdot x_i + b = \varepsilon_i > 0, \quad x_i \in S_+ \\
   w \cdot x_j + b = \varepsilon_j < 0, \quad x_j \in S_-
\end{gather*}
$$

根据凸壳的定义，对于$\text{conv}(S_+)$中的任意元素$s_+$，有  
$$
\begin{aligned}
w \cdot s_+ + b &= w \cdot (\sum_{i=1}^{|S_+|} \lambda_i x_i) + b= (\sum_{i=1}^{|S_+|} \lambda_i(\varepsilon_i - b)) + b \\
&= \sum_{i=1}^{|S_+|} \lambda_i \varepsilon_i - b(\sum_{i=1}^{|S_+|} \lambda_i) + b\\
& = \sum_{i=1}^{|S_+|} \lambda_i \varepsilon_i > 0 
\end{aligned}
$$

同理对于$S_-$中的任意元素$s_-$，有
$$w \cdot s_- + b = \sum_{i=1}^{|S_-|} \lambda_i \varepsilon_i < 0$$
  
若$\text{conv}(S_+)$与$\text{conv}(S_-)$相交，即存在某个元素$s$，同时满足
$$
s \in \text{conv}(S_+)\implies w \cdot s + b = \sum_{i=1}^{|S_+|} \lambda_i \varepsilon_i > 0\\
s \in \text{conv}(S_-) \implies w \cdot s + b = \sum_{i=1}^{|S_-|} \lambda_i \varepsilon_i < 0
$$
两者矛盾，故此点不存在，即$\text{conv}(S_+)$与$\text{conv}(S_-)$不相交，必要性得证。

**充分性**证明：凸壳不相交$\Rightarrow$线性可分

设$\text{conv}(S_+)$与$\text{conv}(S_-)$不相交。   
定义两个点$x_1,x_2$的距离为
$$
\text{dist}(x_1,x_2) = \|x_1 - x_2\|_2
$$  
定义$\text{conv}(S_+)$、$\text{conv}(S_-)$的距离是，分别处于两个凸壳集合中的点的距离最小值：
$$
\text{dist}(\text{conv}(S_+),\text{conv}(S_-)) = \min \|s_+ - s_-\|_2 \quad s_+ \in \text{conv}(S_+), s_- \in \text{conv}(S_-)
$$  
记最小值点分别为$x_+, x_-$，即：
$$
\text{dist}(\text{conv}(S_+),\text{conv}(S_-)) = \text{dist}(x_+, x_-) \quad x_+ \in \text{conv}(S_+), x_- \in \text{conv}(S_-)
$$ 

将以$(x_+, x_-)$为法线，且过两点中点的超平面记为
$$f(x|w,b)=(x_+-x_-)^T(x - \frac{x_+ + x_-}{2})=0$$

设当$x\in \text{conv}(S_+)$时，$f(x)<0$，则有：
$$
\begin{aligned}
f(x)&=(x_+-x_-)^T(x - \frac{x_+ + x_-}{2}) \\
&=(x_+-x_-)^T(x + x_+ - x_+ - \frac{x_+ + x_-}{2}) \\
&=(x_+-x_-)^T(x - x_+ + \frac{x_+ - x_-}{2}) \\
&=(x_+-x_-)^T(x - x_+) + \frac{{\|x_+ - x_-\|_2}^2}{2} <0  \\
 \implies& (x_+-x_-)^T(x - x_+) < 0
\end{aligned}
$$

设点$u=x_++t(x-x_+), t\in [0,1]$，即$u$在$x_+$和$x$的线段上。根据凸壳定义，$u \in \text{conv}(S_+)$。则$u$和$x_-$距离的平方为：
$$g(t)={\|u-x_-\|_2}^2={\|x_++t(x-x_+)-x_-\|_2}^2 $$
为求解$u$和$x_-$距离的最小值，对上式求导：
$$
\begin{aligned}
g'(t)&=2(x_++t(x-x_+)-x_-)(x-x_+) \\
&=2(x_+-x_-)^T(x-x_+)+t{\|x-x_+\|_2}^2
\end{aligned}
$$
根据假设，在$t=0$时，得$g'(t)<0$。在当$t$足够接近于0时，$g(t)<g(0)$。  
故存在点$u$，使得它到$x_-$的距离，比定义的凸壳距离$\text{dist}(x_+,x_-)$还小。产生矛盾。故原命题成立，即$f(x)≥0, x\in \text{conv}(S_+)$。同理，可证$f(x)≤0, x\in \text{conv}(S_-)$。由于凸壳包含了原点集，即此超平面可将原点集分开。充分性得证。
