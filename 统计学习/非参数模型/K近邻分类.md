# K近邻分类器

K 近邻法 (K nearest neighbor, KNN) 是一种[[非参数方法#基于实例模型(exemplar-based model)]] 。基本思想是：对新的实例 $\mathbf{x}$，在训练集 $\mathcal{D}$ 中找到 $k$ 个与其最接近的实例，记为 $N_k(\mathbf{x},\mathcal{D})$，再根据分类决策规则得出新实例的标签。模型的超参数为近邻数 $K$ 、距离度量及分类决策规则。

当训练集、距离度量、 K 值及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。
![[Pasted image 20230219154926.png]]

### 距离度量

常用距离度量方法有：
+ [[L_p距离]]
+ [[Mahalanobis距离]]

## 分类决策规则

一种自然的想法，选用 $N_k(\mathbf{x},\mathcal{D})$ 中最多的标签类别作为输入实例的类，即多数表决规则 (majority voting rule)。模型可记为：
$$ p(y=c|\boldsymbol{x},\mathcal{D})=\frac{1}{K}\sum_{n\in N_K(\boldsymbol{x},\mathcal{D})}\mathbb{I}\left(y_n=c\right) $$




### K 值的选择

当 $K=1$ 时，即返回最近的训练样本类别，此时样本空间被划分为Voronoi镶嵌(Voronoi tessellation)：每个样本点 $\mathbf{x}_n$ 对应着一个区域 $V(\mathbf{x}_n)$，此区域内的任意点与 $\mathbf{x}_n$ 的距离都比与其他样本点的更小，此区域也称为单元(cell)。此时训练集中的各样本点都将被分类为其本身的类，故训练误差为零。

同理，如果选择较小的 $K$ 值，训练误差较小。但预测结果对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。此时，分类边界，容易发生过拟合。

相反，如果选择较大的 $K$ 值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。$K$ 值的增大就意味着整体的模型变得简单。

如果$k = N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。

在应用中，$K$ 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的 $K$ 值。


