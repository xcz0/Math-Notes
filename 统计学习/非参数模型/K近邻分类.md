# K近邻分类器

K 近邻法 (K nearest neighbor, KNN) 是一种[[非参数方法#基于实例模型(exemplar-based model)]] 。基本思想是：对新的实例 $\mathbf{x}$，在训练集 $\mathcal{D}$ 中找到 $k$ 个与其最接近的实例，记为 $N_k(\mathbf{x},\mathcal{D})$，再根据分类决策规则得出新实例的标签。模型的超参数为近邻数 $K$ 、距离度量及分类决策规则。

模型可记为：
$$ p(y=c|\boldsymbol{x},\mathcal{D})=\frac{1}{K}\sum_{n\in N_K(\boldsymbol{x},\mathcal{D})}\mathbb{I}\left(y_n=c\right) $$

K近邻分类器实际上是，利用训练数据集，对特征向量空间进行划分。
![[Pasted image 20230219154926.png]]



## k 近邻模型

k 近邻法中，当训练集、距离度量（如欧氏距离）、 K 值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。



特征空间中，对每个训练实例点x'I,，距离该点比其他点更近的所有点组成一个区域，叫作单元 (cell汃每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例立的类 y'I, 作为其单元中所有点的类标记(class label ) 。这样，每个单元的实例点的类别是确定的。




### K 值的选择

$K$ 值的选择会对 $K$ 近邻法的结果产生重大影响。

如果选择较小的$K$值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差(approximation error)会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差(estimation error)会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，$k$ 值的减小就意味着整体模型变得复杂，容易发生过拟合。

如果选择较大的$K$值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。$K$值的增大就意味着整体的模型变得简单。

如果$k = N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。

在应用中，$K$值一般取一个比较小的数值。通常采用交叉验证法来选取最优的 $K$ 值。

## K近邻算法：kd 树

kd 树是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。 kd 树相当于 K 维空间中的二分查找树(Binary Search Tree，BST)，每个节点判断的数值来自于不同的维度。

![[Pasted image 20230219163512.png]]

![[Pasted image 20230219163525.png]]

### 构造 kd 树

>[!tip] 构造平衡 kd 树
>输入：$K$ 维空间数据集$T = \{x_1, x_{2},\cdots,x_{N}\}$
>输出：kd 树
>1. 构造根结点：
>2. 选取方差最大的特征作为分割特征；
>3. 选择该特征的中位数作为分割点；
>4. 将数据集中该特征小于中位数的传递给根节点的左儿子，大于中位数的传递给根节点的右儿子；
>5. 递归执行步骤2-4，直到所有数据都被建立到KD Tree的节点上为止。

您可能还会问，为什么方差最大的适合作为特征呢？ 因为方差大，数据相对“分散”，选取该特征来对数据集进行分割，数据散得更“开”一些。您可能又要问，为什么选择中位数作为分割点呢？ 因为借鉴了BST，选取中位数，让左子树和右子树的数据数量一致，便于二分查找。

### 搜索 kd 树

>[!tip] kd 树的最近邻搜索
>输入：已构造的 kd 树，目标点$x$;
>输出：$x$的最近邻。
>1. 从根节点开始，根据目标在分割特征中是否小于或大于当前节点，向左或向右移动：
>2. 一旦算法到达叶节点，它就将节点点保存为“当前最佳”；
>3. 回溯，即从叶节点再返回到根节点；
>4. 如果当前节点比当前最佳节点更接近，那么它就成为当前最好的；
>5. 如果目标距离当前节点的父节点所在的将数据集分割为两份的超平面的距离更接近，说明当前节点的兄弟节点所在的子树有可能包含更近的点。因此需要对这个兄弟节点递归执行1-4步。