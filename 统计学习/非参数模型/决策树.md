# 决策树

决策树模型是一种呈树形结构的 [[非参数方法]]，通过递归的方式划分特征空间，让处于同一区域的训练样本点具有相同（或大部分相同）的标签。

决策树的根节点代表整个训练集，根据某一特征，将训练集划分为两个部分（即左右子节点），尽量让同一部分的标签相同。如果已经做到完全相同，则停止划分；否则子节点继续依此原则划分，直到某一节点中的样本标签相同，或者不再有可以划分的特征，就将其作为叶节点。

决策树可以认为是 if-then 规则的集合，本质上是从训练数据集中归纳出一组分类规则。由决策树的根结点到叶结点的每一条路径构建一条规则：路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。每一个实例都被一条路径所覆盖，而且只被一条路径所覆盖。

决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

*优点*：模型大小，模型具有可读性，分类速度快。






## 特征选择

特征选择在千选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。

信息增益 (information gain) 表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度，由训练数据集中类与特征的互信息定义。[[条件熵]]

特征 A 对训练数据集 D 的信息增益 g(D,A) ，定义为集合 D 的经验嫡 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差，即
$$ g(D,A)=H(D)-H(D|A) $$



### 决策树的生成

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进
行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。

### 决策树的修剪

由训练数据生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过千细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。


## 回归树

