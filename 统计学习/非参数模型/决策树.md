# 决策树

决策树模型是一种呈树形结构的 [[非参数方法]]，通过递归的方式划分特征空间，让处于同一区域的训练样本点具有相同（或大部分相同）的标签。

决策树的根节点代表整个训练集，根据某一特征，将训练集划分为两个部分（即左右子节点），尽量让同一部分的标签相同。如果已经做到完全相同，则停止划分；否则子节点继续依此原则划分，直到某一节点中的样本标签相同，或者不再有可以划分的特征，就将其作为叶节点。

决策树可以认为是 if-then 规则的集合，本质上是从训练数据集中归纳出一组分类规则。由决策树的根结点到叶结点的每一条路径构建一条规则：路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。每一个实例都被一条路径所覆盖，而且只被一条路径所覆盖。

决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

*优点*：
+ 不需要储存所有训练样本，树的深度一般为训练数据量的 $O(\log n)$ 级别
+ 可读性强
+ 分类速度快
+ 不需要设置度量

## 决策树的生成

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。

## 特征选择

应选择对训练数据具有“辨识度”的特征，提高决策树学习的效率。如果利用一个特征进行划分的结果与随机均匀划分的结果没有很大差别，那么认为这个特征没有辨识度。好的划分结果应尽可能使同一区域的样本标签相似。样本标签的相似度可以用不纯度函数度量，度量方式产生了不同的决策树算法：

### ID3算法

使用[[熵]]作为不纯度函数，若原样本集 $S$ 经某一特征 $A$ 划分为了左子树集 $S_{L}$ 与右子树集 $S_R$ ，则此划分结果的经验条件熵为
$$H(S|A)=\frac{|S_L|}{|S|}H(S_L)+\frac{|S_R|}{|S|}H(S_R)$$
在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择经验条件熵*最小*的特征及其对应的切分点作为最优特征与最优切分点。当不纯度函数为信息熵时不纯度变化又叫做**信息增益**(info gain).

### C4.5算法

使用，；

### CART算法

使用[[Gini指数]]作为不纯度函数。若原样本集 $S$ 经某一特征 $A$ 划分为了左子树集 $S_{L}$ 与右子树集 $S_R$ ，则此划分结果的Gini指数为
$$G(S,A)=\frac{|S_L|}{|S|}G(S_L)+\frac{|S_R|}{|S|}G(S_R)$$
在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择Gini指数*最小*的特征及其对应的切分点作为最优特征与最优切分点。

## 决策树的修剪

为减少过拟合现象，对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。



