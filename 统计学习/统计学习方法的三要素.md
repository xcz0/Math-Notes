# 统计学习方法的三要素

从给定的、有限的、用于学习的训练数据 (training data) 集合出发，假设数据是独立同分布产生的。
+ 模型 (model) ：即假设空间，模型属于的函数集合
+ 策略 (strategy)：从假设空间中选择最优模型的准则
+ 算法 (algorithm)：依据策略选择模型的方法

实现统计学习方法的步骤如下 ：
1. 得到一个有限的训练数据集合；
2. 确定包含所有可能的模型的假设空间，即学习模型的集合；
3. 确定模型选择的准则，即学习的策略；
4. 实现求解最优模型的算法，即学习的算法；
5. 通过学习方法选择最优模型；
6. 利用学习的最优模型对新数据进行预测或分析。

## 模型

假设空间用 $\mathcal{F}$ 表示。假设空间可以定义为决策函数的集合：
$$\mathcal{F}=\{ f|Y=f(X)\}$$
其中，$X$ 和 $Y$ 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的变量。

假设空间也可以定义为条件概率的集合：
$$\mathcal{F}=\{P|P(Y|X)\}$$
其中，$X$ 和 $Y$ 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的随机变量。

$\mathcal{F}$ 通常是由一个参数向量决定的函数族。参数向量 $\theta$ 取值于n 维欧氏空间 R，称为参数空间 (parameter space)。

## 策略

##### 损失函数和风险函数

> **损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏。

损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(Y,f(X))$。

常用的损失函数有以下几种：
+ 0-1 损失函数( 0-1 loss function)：$L(Y,f(X))=\mathbb{I}(Y \neq f(X))$
+ 平方损失函数 (quadratic loss function)：$L(Y,f(X))=(Y - f(X))^2$
+ 绝对损失函数 (absolute loss function)：$L(Y,f(X))=|Y - f(X)|$
+ 对数损失函数 (logarithmic loss function) 或对数似然损失函数 (log-likelihood loss function)：$L(Y,P(Y|X))=-\log P(Y|X)$，`$L(Y,P(Y|X))$表示若实际输出为$Y$，希望模型$P(y|x)$在已知输入为$X$时，输出为$Y$的可能性$P(Y|X)$越大越好。`

由于模型的输入、输出 (X,Y) 是随机变量，遵循联合分布 P(X,Y) ，所以损失函数的期望是
$$R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y$$
此即模型的风险函数 (risk function) 或期望损失 (expected loss)。

由于联合分布 $P(X,Y)$ 是未知的，$R_{exp}(f)$ 不能直接计算，故用训练集中频率代替概率，得到经验风险(empirical risk) 或经验损失 (empirical loss) ：
$$R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$$

根据大数定律，当样本容量趋于无穷时，经验风险趋于期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略 ： 经验风险最小化和结构风险最小化。

##### 经验风险与结构风险

经验风险最小化 (empirical risk minimization, ERM)的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：
$$\min_{f \in \mathcal{F}}\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$$

当样本容量足够大时，经验风险最小化能保证有很好的学习效果。比如，极大似然估计 (maximum likelihood estimation)就是经验风险最小化的一个例子。当模型是条件概率分布 $P_{\theta}(Y|X)$、损失函数是对数损失函数 $L(Y,P(Y|X)) = -\log P(Y|X)$ 时，问题可转化为：
$$
\begin{aligned}
\mathop{\arg\min} \limits_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) &= \mathop{\arg\min} \limits_{f \in \mathcal{F}} \frac{1}{N} \sum_D [-\log P(Y|X)] \\
&= \mathop{\arg\max} \limits_{f \in \mathcal{F}} \frac{1}{N} \sum_D \log P(Y|X) \\
&= \mathop{\arg\max} \limits_{f \in \mathcal{F}} \frac{1}{N} \log \prod_D P(Y|X) \\
&= \frac{1}{N} \mathop{\arg\max} \limits_{f \in \mathcal{F}} \log \prod_D P(Y|X)
\end{aligned}
$$
此即极大似然估计。

但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟
合 “(Cover-fitting) 现象。结构风险最小化 (structural risk minimization, SRM) 是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化 (regularization ) 。结构风险在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或罚项 (penalty term) 。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是：
$$R_{srm}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))+\lambda J(f)$$
其中 $J(f)$ 为模型的复杂度，模型 $f$ 越复杂，复杂度 $J(f)$ 就越大。$\lambda >0$ 是系数，用以权衡经验风险和模型复杂度。

结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优模型，就是求解最优化问题：
$$\min_{f \in \mathcal{F}}\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))+\lambda J(f)$$

贝叶斯估计中的最大后验概率估计(maximum posterior probability esti-mation, MAP) 就是结构风险最小化的一个例子。当模型是条件概率分布 $P_{\theta}(Y|X)$、损失函数是对数损失函数 $L(Y,P(Y|X)) = -\log P(Y|X)$、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

## 算法

#### 伯努利模型中的统计学习方法三要素

##### 伯努利模型

伯努利模型属于无监督学习，输入空间为 $\mathcal{X}=\{0,1\}$。模型表示为条件概率：
$$P_p(x|p)=p^x(1-p)^{(1-x)}$$
其假设空间为：
$$\mathcal{F}=\{P|P_p(x|p)=p^x(1-p)^{(1-x)},p \in [0,1]\}$$

##### 经验风险最小化(极大似然估计)

若采用对数损失函数，并要求经验风险最小化，即相当于求解最优化问题：
$$\min_{f \in \mathcal{F}}\frac{1}{N}\sum^{N}_{i=1}L(y_i,P(y_i|x_i))=\max_{p \in [0,1]}\frac{1}{n}\sum^{n}_{i=1}\log P_p(x_i|p)$$

假设数据 $X$ 中有 $k$ 个为 $1$，则
$$L(p)=\sum^{n}_{i=1}\log P_p(x_i|p)=\log \prod^{n}_{i=1}P_p(x_i|p)=\log(p^k(1-p)^{n-k})=k \log p+(n-k)\log(1-p)$$

对参数 $p$ 求导，并求解导数为 $0$ 时的 $p$ 值：
$$
\begin{aligned}
\frac{\partial \log L(p)}{\partial p} &= \frac{k}{p} - \frac{n-k}{1-p} \\
&= \frac{k(1-p) - p(n-k)}{p(1-p)} \\
&= \frac{k-np}{p(1-p)}
\end{aligned}
$$
令 $\frac{\partial \log L(p)}{\partial p} = 0$，从上式可得 $p=\frac{k}{n}$ ，此策略下的算法为 $P(X=1)=\frac{k}{n}$。

##### 结构风险最小化(贝叶斯估计)

若采用对数损失函数，并要求结构风险最小化，模型复杂度由模型的先验概率表示（$J(f)=-\log p^\alpha(1-p)^\beta$），即相当于求解最优化问题：
$$\max_{p \in [0,1]}\frac{1}{n}\sum^{n}_{i=1}\log P_p(x_i|p)+\log p^\alpha(1-p)^\beta$$

假设数据 $X$ 中有 $k$ 个为 $1$，则
$$L(p)=\sum^{n}_{i=1}\log P_p(x_i|p)+\log p^\alpha(1-p)^\beta=\log \prod^{n}_{i=1}P_p(x_i|p)p^\alpha(1-p)^\beta=\log(p^k(1-p)^{n-k})=(k+\alpha) \log p+(n-k+\beta)\log(1-p)$$

对参数 $p$ 求导，并求解导数为 $0$ 时的 $p$ 值：
$$
\begin{aligned}
\frac{\partial \log L(p)}{\partial p} &= \frac{k+\alpha}{p} - \frac{n-k+\beta}{1-p} \\
&= \frac{(k+\alpha)(1-p) - p(n-k+\beta)}{p(1-p)} \\
&= \frac{k+\alpha-(n+\alpha+\beta)p}{p(1-p)}
\end{aligned}
$$
令 $\frac{\partial \log L(p)}{\partial p} = 0$，从上式可得 $p=\frac{k+\alpha}{n+\alpha+\beta}$ ，此策略下的算法为 $P(X=1)=\frac{k+\alpha}{n+\alpha+\beta}$。

