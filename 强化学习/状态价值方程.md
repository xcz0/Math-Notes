# 状态价值方程

由于 $G_t=R_{t+1}+\gamma G_{t+1}$，所以：
$$\begin{aligned}
v_{\pi}(s)& =\mathbb{E}[G_t|S_t=s] \\
&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&=\mathbb{E}[R_{t+1}|S_t=s]+\gamma\mathbb{E}[G_{t+1}|S_t=s]
\end{aligned}$$
即状态价值可拆分为即时回报加上衰减的未来回报。

其中即时奖励的均值为：
$$\begin{aligned}
\mathbb{E}[R_{t+1}|S_{t}=s]& =\sum_a\pi(a|s)\mathbb{E}[R_{t+1}|S_t=s,A_t=a] \\
&=\sum_a\pi(a|s)\sum_rp(r|s,a)r
\end{aligned}$$

未来回报的均值为：
$$\begin{aligned}
\mathbb{E}[G_{t+1}|S_{t}=s]& =\sum_{s'}\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']p(s'|s) \\
&=\sum_{s'}\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s) \\
&=\sum_{s'}v_\pi(s')p(s'|s) \\
&=\sum_{s'}v_\pi(s')\sum_ap(s'|s,a)\pi(a|s)
\end{aligned}$$
注意第二步使用了无记忆性。

综合得：
$$\begin{align}
v_{\pi}(s) & = \mathbb{E}[R_{t+1}|S_t=s]+\gamma\mathbb{E}[G_{t+1}|S_t=s] \\
&= \sum_a\pi(a|s)\sum_rp(r|s,a)r + \gamma \sum_a\pi(a|s)\sum_{s'}p(s'|s,a)v_\pi(s') \\
&= \sum_a\pi(a|s)\left[\sum_rp(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)v_\pi(s') \right]
\end{align}$$
称此方程为**Bellman方程**。若环境模型 $p(r|s,a),p(s'|s,a)$ 是确定的，则此状态函数只与策略 $\pi(a|s)$ 相关，求解此方程称为策略评估。

记
$$r_\pi(s)\triangleq\sum_a\pi(a|s)\sum_rp(r|s,a)r,\quad p_\pi(s'|s)\triangleq\sum_a\pi(a|s)p(s'|s,a)$$
则原方程可改写为
$$v_\pi(s)=r_\pi(s)+\gamma\sum_{s'}p_\pi(s'|s)v_\pi(s')$$
将其改写为矩阵形式
$$ \mathbf{v}_{\pi}=\mathbf{r}_{\pi}+\gamma P_{\pi}\mathbf{v}_{\pi} $$
其中
$$\begin{align}
v_{\pi}&=[v_{\pi}(s_{1}),\ldots,v_{\pi}(s_{n})]^{T} \\
r_{\pi}&=[r_{\pi}(s_{1}),\ldots,r_{\pi}(s_{n})]^{T} \\
[P_\pi]_{ij}&=p_\pi(s_j|s_i)
\end{align}$$
注意到 $P_{\pi}$ 实际上为 [[Markov过程#状态转移矩阵]]。

## 状态价值方程的解析解

其解析解为
$$  \mathbf{v}_{\pi}=(I-\gamma P_{\pi})^{-1}\mathbf{r}_{\pi} $$

#TODO 参考MFRL书p27，第2.7.1节

## 状态价值方程的迭代求解

记 $f(x)=\mathbf{r}_{\pi}+\gamma P_{\pi}x$，则在n维实数空间中
$$ \mathrm{d}(f(x),f(y)) = \gamma \|P_{\pi}(x-y)\|\leq\gamma \|P_{\pi}\| \|x-y\|$$
而由于 $P_{\pi}$ 的概率意义，故 $P_{\pi}\mathbf{1}=\mathbf{1}$，即

#TODO 参考MFRL书p28，第2.7.2节