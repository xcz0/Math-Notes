# 强化学习

强化学习给定的训练数据是动态的，而非一开始就给出所有的训练数据的学习方式。程序在⼀系列的时间步骤上与环境交互。在每个特定时间点，程序从环境接收⼀些观察（observation），并且必须选择⼀个动作（action），然后通过某种机制（有时称为执⾏器）将其传输回环境，最后程序从环境中获得奖励（reward）。此后新⼀轮循环开始，agent 接收后续观察，并选择后续操作，依此类推。案例：围棋程序通过不断与自己下棋获取新的数据，并依此学习。

强化学习(reinforcement learning) 是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错 (trial and error) ，以达到学习最优策略的目的。

在每一步 $t$, 智能系统从环境中观测到一个状态 (state) $s_t$ 与一个奖励 (reward) $r_t$, 采取一个动作 (action) $a_t$ 。环境根据智能系统选择的动作，决定下一步 $t+1$ 的状态 $S_{t+1}$ 与奖励 $r_{t+1}$ 。

![[Pasted image 20230214212339.png]]

+ 策略 $\pi$：为给定状态下动作的函数 $a=f(s)$ 或者条件概率分布 $P(a|s)$。
+ 价值函数(valuefunction)或状态价值函数(statevalue function)：策略 $\pi$ 从某一个状态 $s$ 开始的长期累积奖励的数学期望：
$$ v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots]$$
+ 动作价值函数(actionvalue function)定义为策略 $T$ 的从某一个状态 $s$ 和动作 $a$ 开始的长期累积奖励的数学期望：
$$ q_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots|s_t = s, a_t = a]$$

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略 $\pi^*$，$\gamma$ 表示未来的奖励会有衰减。


## 强化学习的分类

强化学习方法中有基于策略的 (policy-based) 、基于价值的 (value-based) ，这两
者属于无模型的( model-free) 方法，还有有模型的( model-based) 方法。

+ 有模型的方法：直接学习马尔可夫决策过程的模型，包括转移概率函数 $P(s'|s, a)$ 和奖励函数 $r(s, a)$。通过模型对环境的反馈进行预测，求出价值函数最大的策略 $\pi^*$。
+ 无模型的、基于策略的方法：不直接学习模型，而是试图求解最优策略 $\pi^*$，表示为函数 $a = f^*(s)$ 或者是条件概率分布 $P^*(a|s)$。通常从一个具体策略开始，通过搜索更优的策略进行。
+ 无模型的、基于价值的方法：不直接学习模型，而是试图求解最优价值函数，特别是最优动作价值函数 $q^*(s, a)$ 。通常从一个具体价值函数开始，通过搜索更优的价值函数进行。

