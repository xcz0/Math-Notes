# 强化学习(reinforcement learning)

强化学习给定的训练数据是动态的，而非一开始就给出所有的训练数据的学习方式。程序在⼀系列的时间步骤上与环境交互。在每个特定时间点，程序从环境接收⼀些观察（observation），并且必须选择⼀个动作（action），然后通过某种机制（有时称为执⾏器）将其传输回环境，最后程序从环境中获得奖励（reward）。此后新⼀轮循环开始，agent 接收后续观察，并选择后续操作，依此类推。*智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化*。强化学习过程中，系统不断地试错 (trial and error) ，以达到学习最优策略的目的。案例：围棋程序通过不断与自己下棋获取新的数据，并依此学习。

## 基础概念

**状态**(State)：智能体在环境中的位置，所有状态的集合称为**状态空间**(State space)：$\mathcal{S}=\{ s_i \}_{i=1}^n$。

**动作**(action)：在某个状态下，向另一个状态的转移过程，所有可能的动作称为某一状态下的动作空间(Action space of a state)： $\mathcal{A}(s_i)=\{ a_i \}_{j=1}^m$。状态转移(State transition)记为
$$s_1\xrightarrow{a_2}s_2$$
这也可以用概率描述：
$$ \begin{align}
p(s_2|s_1,a_2)&=1 \\
p(s_i|s_1,a_2)&=0 \quad \forall i \neq 2
\end{align} $$
此时，动作 $a_2$ 是确定性的(deterministic)，若移至其他状态的概率皆小于1，则称之为随机的(stochastic)。

**策略**(Policy)：对于所有可能的状态，都分配一个动作。用概率描述：
$$ \begin{align}
\pi(a_2|s_1)&=1 \\
\pi(a_i|s_1)&=0 \quad \forall i \neq 2
\end{align} $$
同样，若其他动作的可能可以不为0。

**奖励**(reward)：在特定状态下，执行特定动作后，得到的实数。用概率描述：
$$ \begin{align}
p(r=-1|s_1,a_1)&=1 \\
p(r\neq-1|s_1,a_1)&=0 
\end{align} $$
同样，这可以是随机的。注意：奖励与状态动作相关，而非下一个动作。

**轨迹**(trajectory)：一条状态动作奖励链：
$$s_1\xrightarrow[r=0]{a_2}s_2\xrightarrow[r=0]{a_3}s_5\xrightarrow[r=0]{a_3}s_8\xrightarrow[r=1]{a_2}s_9$$
一条轨迹上的所有奖励之和称为**回报**(return)。回报用于评价轨迹的好坏，是强化学习的优化目标函数。

为一般化考虑，假设轨迹是无限长的。为避免回报发散，设置奖励随时间衰减，衰减比例记为 $\gamma \in [0,1)$，


在每一步 $t$, 智能系统从环境中观测到一个状态 (state) $s_t$ 与一个奖励 (reward) $r_t$, 采取一个动作 (action) $a_t$ 。环境根据智能系统选择的动作，决定下一步 $t+1$ 的状态 $S_{t+1}$ 与奖励 $r_{t+1}$ 。

![[Pasted image 20230214212339.png]]

+ 策略 $\pi$：为给定状态下动作的函数 $a=f(s)$ 或者条件概率分布 $P(a|s)$。
+ 价值函数(valuefunction)或状态价值函数(statevalue function)：策略 $\pi$ 从某一个状态 $s$ 开始的长期累积奖励的数学期望：
$$ v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots]$$
+ 动作价值函数(actionvalue function)定义为策略 $T$ 的从某一个状态 $s$ 和动作 $a$ 开始的长期累积奖励的数学期望：
$$ q_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots|s_t = s, a_t = a]$$

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略 $\pi^*$，$\gamma$ 表示未来的奖励会有衰减。


## 强化学习的分类

强化学习方法中有基于策略的 (policy-based) 、基于价值的 (value-based) ，这两
者属于无模型的( model-free) 方法，还有有模型的( model-based) 方法。

+ 有模型的方法：直接学习马尔可夫决策过程的模型，包括转移概率函数 $P(s'|s, a)$ 和奖励函数 $r(s, a)$。通过模型对环境的反馈进行预测，求出价值函数最大的策略 $\pi^*$。
+ 无模型的、基于策略的方法：不直接学习模型，而是试图求解最优策略 $\pi^*$，表示为函数 $a = f^*(s)$ 或者是条件概率分布 $P^*(a|s)$。通常从一个具体策略开始，通过搜索更优的策略进行。
+ 无模型的、基于价值的方法：不直接学习模型，而是试图求解最优价值函数，特别是最优动作价值函数 $q^*(s, a)$ 。通常从一个具体价值函数开始，通过搜索更优的价值函数进行。

