# 状态价值

状态价值：从特定状态出发，依特定策略获得的回报。

考虑单步过程：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}$$ 皆为随机变量，其中
+ $S_t \to A_t$ 由策略 $\pi(A_t=a|S_t=s)$ 描述
+ $S_t,A_t \to S_{t+1}$ 由动作模型 $p(S_{t+1}=s'|S_t=s,A_t=a)$ 描述
+ $S_t,A_t \to R_{t+1}$ 由奖励模型 $p(R_{t+1}=r|S_t=s,A_t=a)$ 描述

对于多步轨迹：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}\xrightarrow{A_{t+1}}R_{t+2},S_{t+2}\xrightarrow{A_{t+2}}R_{t+3},\ldots $$
其折扣回报为
$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots $$ 
>随机变量 $G_t$ 虽然受 $S_t$ 取值的影响，但其直接定义与 $S_t$ 无关，仅与 $t$ 相关

其期望定义为状态价值函数
$$ v_\pi(s)=\mathbb{E}[G_t|S_t=s]$$
当所有模型为决定性而非随机的，状态价值即为回报。


## 状态价值方程

由于
$$\begin{align}
G_t& =R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots \\
&=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots) \\
&=R_{t+1}+\gamma G_{t+1}
\end{align}$$
故
