# 状态价值

状态价值：从特定状态出发，依特定策略获得的回报。

考虑单步过程：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}$$ 皆为随机变量，其中
+ $S_t \to A_t$ 由策略 $\pi(A_t=a|S_t=s)$ 描述
+ $S_t,A_t \to S_{t+1}$ 由动作模型 $p(S_{t+1}=s'|S_t=s,A_t=a)$ 描述
+ $S_t,A_t \to R_{t+1}$ 由奖励模型 $p(R_{t+1}=r|S_t=s,A_t=a)$ 描述

对于多步轨迹：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}\xrightarrow{A_{t+1}}R_{t+2},S_{t+2}\xrightarrow{A_{t+2}}R_{t+3},\ldots $$
其折扣回报为
$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots $$ 
>随机变量 $G_t$ 虽然受 $S_t$ 取值的影响，但其直接定义与 $S_t$ 无关，仅与 $t$ 相关

其期望定义为状态价值函数
$$ v_\pi(s)=\mathbb{E}[G_t|S_t=s]$$
当所有模型为决定性而非随机的，状态价值即为回报。


## 状态价值方程

由于
$$\begin{align}
G_t& =R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots \\
&=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots) \\
&=R_{t+1}+\gamma G_{t+1}
\end{align}$$
故
$$\begin{aligned}
v_{\pi}(s)& =\mathbb{E}[G_t|S_t=s] \\
&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&=\mathbb{E}[R_{t+1}|S_t=s]+\gamma\mathbb{E}[G_{t+1}|S_t=s]
\end{aligned}$$
即状态价值可拆分为即时回报加上衰减的未来回报。



其中即时奖励的均值为：
$$\begin{aligned}
\mathbb{E}[R_{t+1}|S_{t}=s]& =\sum_a\pi(a|s)\mathbb{E}[R_{t+1}|S_t=s,A_t=a] \\
&=\sum_a\pi(a|s)\sum_rp(r|s,a)r
\end{aligned}$$

未来回报的均值为：
$$\begin{aligned}
\mathbb{E}[G_{t+1}|S_{t}=s]& =\sum_{s'}\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']p(s'|s) \\
&=\sum_{s'}\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s) \\
&=\sum_{s'}v_\pi(s')p(s'|s) \\
&=\sum_{s'}v_\pi(s')\sum_ap(s'|s,a)\pi(a|s)
\end{aligned}$$
注意第二步使用了无记忆性。

综合：
$$\begin{aligned}
v_{\pi}(s) & = \mathbb{E}[R_{t+1}|S_t=s]+\gamma\mathbb{E}[G_{t+1}|S_t=s] \\
&= \sum_a\pi(a|s)\sum_rp(r|s,a)r + \gamma \sum_{s'}v_\pi(s')\sum_ap(s'|s,a)\pi(a|s)
\end{aligned}  $$


